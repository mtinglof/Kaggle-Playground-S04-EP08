{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":76727,"databundleVersionId":9045607,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Resources \n## Links \n* https://www.kaggle.com/code/satyaprakashshukl/mushroom-classification-analysis/notebook\n* https://www.kaggle.com/code/ambrosm/pss4e8-eda-which-makes-sense#First-observations \n* https://www.kaggle.com/code/annastasy/ps4e8-data-cleaning-and-eda-of-mushrooms\n\n## Code Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-08-15T21:32:00.393934Z","iopub.execute_input":"2024-08-15T21:32:00.394349Z","iopub.status.idle":"2024-08-15T21:32:00.399935Z","shell.execute_reply.started":"2024-08-15T21:32:00.394317Z","shell.execute_reply":"2024-08-15T21:32:00.398699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Problem Definition\n## 1.1 Scenario\n\nWelcome back! Today, our client has given us the task of binary classification of mushrooms. The client lives far out in a magical forest and needs an application to help her identify which mushrooms she can eat and which she cannot. Luckily for us, she brought with her an ancient tome of mushroom classification. There is some information missing though, so we will have to work around that. She is hoping that we can come up with a model, so she does not have to flip through every page of this massive book. Oh, and she's hoping that your model will not get her sick either! \n\n## 1.2 Scoring Metric\nFor our client, we will be using the Matthews correlation coefficient (MCC). What does this mean? The MCC is a scoring equation that is calculated using true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). For you nerds, here is the full equation. \n\n$$\n\\text{MCC} = \\frac{(TP \\times TN) - (FP \\times FN)}{\\sqrt{(TP + FP) \\times (TP + FN) \\times (TN + FP) \\times (TN + FN)}}\n$$\n\nThe MCC is great for binary classification, especially where datasets are imbalanced in favor of one type of classification. The last thing to know is that this equation will give us a score from -1 to 1, where -1 is total misclassification, 0 is no predictive power (randomly guessing), and 1 is a perfect classification. \n\n# 2. Data Collection and Analysis\n## 2.1 Import Data and Look at Raw Reads\nAlright, let's look at this data that our client has collect from her massive tome.  ","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/playground-series-s4e8/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/playground-series-s4e8/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T21:43:25.370053Z","iopub.execute_input":"2024-08-15T21:43:25.370441Z","iopub.status.idle":"2024-08-15T21:43:38.205277Z","shell.execute_reply.started":"2024-08-15T21:43:25.370411Z","shell.execute_reply":"2024-08-15T21:43:38.204361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T19:53:18.869027Z","iopub.execute_input":"2024-08-15T19:53:18.869324Z","iopub.status.idle":"2024-08-15T19:53:18.902595Z","shell.execute_reply.started":"2024-08-15T19:53:18.869299Z","shell.execute_reply":"2024-08-15T19:53:18.901543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T19:53:18.904638Z","iopub.execute_input":"2024-08-15T19:53:18.904987Z","iopub.status.idle":"2024-08-15T19:53:18.929042Z","shell.execute_reply.started":"2024-08-15T19:53:18.904935Z","shell.execute_reply":"2024-08-15T19:53:18.927776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Visualizations\nThe biggest problem we are going to have with this dataset is the number of artifacts that can be found in each column. ","metadata":{}},{"cell_type":"code","source":"unique_counts = {}\n\nfor column in train_df.columns:\n    unique_counts[column] = train_df[column].nunique()\n\nunique_counts_series = pd.Series(unique_counts)\n\nprint(unique_counts_series)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T19:53:18.930356Z","iopub.execute_input":"2024-08-15T19:53:18.931117Z","iopub.status.idle":"2024-08-15T19:53:20.502272Z","shell.execute_reply.started":"2024-08-15T19:53:18.931086Z","shell.execute_reply":"2024-08-15T19:53:20.501137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the table above, we see that there are quite a few unique values per column. In addition, it does not seem that the data dictionary found here or here correlates exactly with our dataset. While it seems very close, both data dictionaries do not seem to describe all variables in our dataset. Below, lets see what percentage each value makes in each column.","metadata":{}},{"cell_type":"code","source":"def calculate_category_percentages(df):\n    for column in df.columns:\n        if df[column].dtype == 'object':\n            print(f\"Column: {column}\")\n            percentage = df[column].value_counts(normalize=True) * 100\n            print(f\"{percentage.to_string()}\\n\")\n\ncalculate_category_percentages(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T21:27:11.293313Z","iopub.execute_input":"2024-08-15T21:27:11.294116Z","iopub.status.idle":"2024-08-15T21:27:14.838529Z","shell.execute_reply.started":"2024-08-15T21:27:11.294077Z","shell.execute_reply":"2024-08-15T21:27:14.837523Z"},"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we are trying to reduce these values per column, a good cutoff looks like any value that is < .1% of the values in a column. We will handle that in our data cleaning section. ","metadata":{}},{"cell_type":"code","source":"null_counts = train_df.isnull().sum()\nnull_counts = null_counts.sort_values(ascending=False)\n\nprint(null_counts)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=null_counts.index, y=null_counts.values, palette='viridis')\n\nplt.title('Number of Null Values in Each Column')\nplt.xlabel('Column Names')\nplt.ylabel('Number of Null Values')\nplt.xticks(rotation=90)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T19:53:20.503562Z","iopub.execute_input":"2024-08-15T19:53:20.503879Z","iopub.status.idle":"2024-08-15T19:53:23.329735Z","shell.execute_reply.started":"2024-08-15T19:53:20.503849Z","shell.execute_reply":"2024-08-15T19:53:23.328531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"duplicate_count = train_df.duplicated().sum()\n\nprint(f\"Number of duplicate rows: {duplicate_count}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T19:57:27.407993Z","iopub.execute_input":"2024-08-15T19:57:27.408660Z","iopub.status.idle":"2024-08-15T19:57:31.615140Z","shell.execute_reply.started":"2024-08-15T19:57:27.408627Z","shell.execute_reply":"2024-08-15T19:57:31.614110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 First Thoughts\n* We have a **ton** of training data. Good news for us, we will be able to split this into a train, dev, test set. \n* There are a lot of data artifacts in the set. In the next section, we will define .1% as the cutoff for artifacts that will get reassigned to another value. \n* Null values will also have to be handled. \n* No duplicates, that's a good sign. \n\n# 3. Data Cleaning\nNow that we have a good sense of what data we are working with, let's get it ready for a model. First, let's handle those data artifacts by writing a function to rewrite those smaller values and any null values we might have. Intuitively we know already that we are probably going to be using a Deep Neural Network (DNN) for this problem. ","metadata":{}},{"cell_type":"code","source":"def rewrite_values(df, threshold=0.1):\n    for column in df.columns:\n        if df[column].dtype == 'object':\n            total_count = df[column].notna().sum()\n            value_counts = df[column].value_counts()\n            replace_dict = {value: 'not' if count / total_count < (threshold / 100) else value\n                            for value, count in value_counts.items()}\n            df[column] = df[column].map(replace_dict).fillna('null')\n\nrewrite_values(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T21:43:38.207007Z","iopub.execute_input":"2024-08-15T21:43:38.207668Z","iopub.status.idle":"2024-08-15T21:43:50.197991Z","shell.execute_reply.started":"2024-08-15T21:43:38.207629Z","shell.execute_reply":"2024-08-15T21:43:50.197021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_counts = {}\n\nfor column in train_df.columns:\n    unique_counts[column] = train_df[column].nunique()\n\nunique_counts_series = pd.Series(unique_counts)\n\nprint(unique_counts_series)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T21:44:02.015648Z","iopub.execute_input":"2024-08-15T21:44:02.016076Z","iopub.status.idle":"2024-08-15T21:44:05.167697Z","shell.execute_reply.started":"2024-08-15T21:44:02.016041Z","shell.execute_reply":"2024-08-15T21:44:05.166669Z"},"trusted":true},"execution_count":null,"outputs":[]}]}