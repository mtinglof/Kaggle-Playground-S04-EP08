{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":76727,"databundleVersionId":9045607,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Resources \n## Links \n* https://www.kaggle.com/code/satyaprakashshukl/mushroom-classification-analysis/notebook\n* https://www.kaggle.com/code/ambrosm/pss4e8-eda-which-makes-sense#First-observations \n* https://www.kaggle.com/code/annastasy/ps4e8-data-cleaning-and-eda-of-mushrooms\n\n## Code Imports","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:35:00.632194Z","iopub.execute_input":"2024-08-19T20:35:00.632609Z","iopub.status.idle":"2024-08-19T20:35:06.086038Z","shell.execute_reply.started":"2024-08-19T20:35:00.632571Z","shell.execute_reply":"2024-08-19T20:35:06.084612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Problem Definition\n## 1.1 Scenario\n\nWelcome back! Today, our client has given us the task of binary classification of mushrooms. The client lives far out in a magical forest and needs an application to help her identify which mushrooms she can eat and which she cannot. Luckily for us, she brought with her an ancient tome of mushroom classification. There is some information missing though, so we will have to work around that. She is hoping that we can come up with a model, so she does not have to flip through every page of this massive book. Oh, and she's hoping that your model will not get her sick either! \n\n## 1.2 Scoring Metric\nFor our client, we will be using the Matthews correlation coefficient (MCC). What does this mean? The MCC is a scoring equation that is calculated using true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN). For you nerds, here is the full equation. \n\n$$\n\\text{MCC} = \\frac{(TP \\times TN) - (FP \\times FN)}{\\sqrt{(TP + FP) \\times (TP + FN) \\times (TN + FP) \\times (TN + FN)}}\n$$\n\nThe MCC is great for binary classification, especially where datasets are imbalanced in favor of one type of classification. The last thing to know is that this equation will give us a score from -1 to 1, where -1 is total misclassification, 0 is no predictive power (randomly guessing), and 1 is a perfect classification. \n\n# 2. Data Collection and Analysis\n## 2.1 Import Data and Look at Raw Reads\nAlright, let's look at this data that our client has collect from her massive tome.  ","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/playground-series-s4e8/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/playground-series-s4e8/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:35:06.093118Z","iopub.execute_input":"2024-08-19T20:35:06.094149Z","iopub.status.idle":"2024-08-19T20:35:24.393285Z","shell.execute_reply.started":"2024-08-19T20:35:06.094113Z","shell.execute_reply":"2024-08-19T20:35:24.391729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:35:24.394805Z","iopub.execute_input":"2024-08-19T20:35:24.395148Z","iopub.status.idle":"2024-08-19T20:35:24.429426Z","shell.execute_reply.started":"2024-08-19T20:35:24.395120Z","shell.execute_reply":"2024-08-19T20:35:24.428271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:35:24.432559Z","iopub.execute_input":"2024-08-19T20:35:24.432963Z","iopub.status.idle":"2024-08-19T20:35:24.448674Z","shell.execute_reply.started":"2024-08-19T20:35:24.432923Z","shell.execute_reply":"2024-08-19T20:35:24.447338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.2 Visualizations\nThe biggest problem we are going to have with this dataset is the number of artifacts that can be found in each column. ","metadata":{}},{"cell_type":"code","source":"unique_counts = {}\n\nfor column in train_df.columns:\n    unique_counts[column] = train_df[column].nunique()\n\nunique_counts_series = pd.Series(unique_counts)\n\nprint(unique_counts_series)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:35:24.450178Z","iopub.execute_input":"2024-08-19T20:35:24.450580Z","iopub.status.idle":"2024-08-19T20:35:27.597431Z","shell.execute_reply.started":"2024-08-19T20:35:24.450537Z","shell.execute_reply":"2024-08-19T20:35:27.595963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the table above, we see that there are quite a few unique values per column. In addition, it does not seem that the data dictionary found [here](https://www.kaggle.com/competitions/playground-series-s4e8/data) or [here](https://archive.ics.uci.edu/dataset/848/secondary+mushroom+dataset) correlates exactly with our dataset. While it seems very close, both data dictionaries do not seem to describe all variables in our dataset. Below, lets see what percentage each value makes in each column.","metadata":{}},{"cell_type":"code","source":"def calculate_category_percentages(df):\n    for column in df.columns:\n        if df[column].dtype == 'object':\n            print(f\"Column: {column}\")\n            percentage = df[column].value_counts(normalize=True) * 100\n            print(f\"{percentage.to_string()}\\n\")\n\ncalculate_category_percentages(train_df)","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-19T20:35:27.599194Z","iopub.execute_input":"2024-08-19T20:35:27.599698Z","iopub.status.idle":"2024-08-19T20:35:33.831962Z","shell.execute_reply.started":"2024-08-19T20:35:27.599645Z","shell.execute_reply":"2024-08-19T20:35:33.830587Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we are trying to reduce these values per column, a good cutoff looks like any value that is < .1% of the values in a column. We will handle that in our data cleaning section. ","metadata":{}},{"cell_type":"code","source":"null_counts = train_df.isnull().sum()\nnull_counts = null_counts.sort_values(ascending=False)\n\nprint(null_counts)\n\nplt.figure(figsize=(10, 6))\nsns.barplot(x=null_counts.index, y=null_counts.values, palette='viridis')\n\nplt.title('Number of Null Values in Each Column')\nplt.xlabel('Column Names')\nplt.ylabel('Number of Null Values')\nplt.xticks(rotation=90)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:35:33.833315Z","iopub.execute_input":"2024-08-19T20:35:33.833665Z","iopub.status.idle":"2024-08-19T20:35:39.306516Z","shell.execute_reply.started":"2024-08-19T20:35:33.833635Z","shell.execute_reply":"2024-08-19T20:35:39.304971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"duplicate_count = train_df.duplicated().sum()\n\nprint(f\"Number of duplicate rows: {duplicate_count}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:35:39.308068Z","iopub.execute_input":"2024-08-19T20:35:39.308492Z","iopub.status.idle":"2024-08-19T20:35:48.802518Z","shell.execute_reply.started":"2024-08-19T20:35:39.308460Z","shell.execute_reply":"2024-08-19T20:35:48.801318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n\nfor i, col in enumerate(train_df.select_dtypes(include=['float64']).columns):\n    axes[i].hist(train_df[col], bins=30, color='skyblue', edgecolor='black')\n    axes[i].set_title(f'Histogram of {col}')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:35:48.804046Z","iopub.execute_input":"2024-08-19T20:35:48.804431Z","iopub.status.idle":"2024-08-19T20:35:49.921275Z","shell.execute_reply.started":"2024-08-19T20:35:48.804400Z","shell.execute_reply":"2024-08-19T20:35:49.919996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 First Thoughts\n* We have a **ton** of training data. Good news for us, we will be able to split this into a train, dev, test set. \n* There are a lot of data artifacts in the set. In the next section, we will define .1% as the cutoff for artifacts that will get reassigned to another value. \n* Null values will also have to be handled. \n* No duplicates, that's a good sign. \n\n# 3. Data Cleaning\n## 3.1 Replacing Null and Artifacts\nNow that we have a good sense of what data we are working with, let's get it ready for a model. First, let's handle those data artifacts by writing a function to rewrite those smaller values and any null values we might have. Intuitively we know already that we are probably going to be using a Deep Neural Network (DNN) for this problem. ","metadata":{}},{"cell_type":"code","source":"for col in train_df.select_dtypes(include=['float64']).columns:\n    train_df[col] = train_df[col].fillna(0)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:36:22.636771Z","iopub.execute_input":"2024-08-19T20:36:22.637312Z","iopub.status.idle":"2024-08-19T20:36:22.705681Z","shell.execute_reply.started":"2024-08-19T20:36:22.637277Z","shell.execute_reply":"2024-08-19T20:36:22.704468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def rewrite_values(df, threshold=0.1):\n    for column in df.columns:\n        if df[column].dtype == 'object':\n            total_count = df[column].notna().sum()\n            value_counts = df[column].value_counts()\n            replace_dict = {value: 'not' if count / total_count < (threshold / 100) else value\n                            for value, count in value_counts.items()}\n            df[column] = df[column].map(replace_dict).fillna('null')\n\nrewrite_values(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:36:37.811079Z","iopub.execute_input":"2024-08-19T20:36:37.812804Z","iopub.status.idle":"2024-08-19T20:37:01.125800Z","shell.execute_reply.started":"2024-08-19T20:36:37.812751Z","shell.execute_reply":"2024-08-19T20:37:01.124464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_counts = {}\n\nfor column in train_df.columns:\n    unique_counts[column] = train_df[column].nunique()\n\nunique_counts_series = pd.Series(unique_counts)\n\nprint(unique_counts_series)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:37:01.127785Z","iopub.execute_input":"2024-08-19T20:37:01.128163Z","iopub.status.idle":"2024-08-19T20:37:06.140620Z","shell.execute_reply.started":"2024-08-19T20:37:01.128132Z","shell.execute_reply":"2024-08-19T20:37:06.139342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.2 Prepare Data for Model","metadata":{}},{"cell_type":"code","source":"scaler = MinMaxScaler()\n\ndf_encoded = pd.get_dummies(train_df, columns=train_df.select_dtypes(include=['object']).columns, dtype=int)\ncontinuous_cols = df_encoded.select_dtypes(include=['float64']).columns\ndf_encoded[continuous_cols] = scaler.fit_transform(df_encoded[continuous_cols])\n\ndf_encoded = df_encoded.drop(['id'], axis=1)\n\npd.set_option('display.max_columns', None)\ndf_encoded.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:37:06.142130Z","iopub.execute_input":"2024-08-19T20:37:06.142533Z","iopub.status.idle":"2024-08-19T20:37:24.519937Z","shell.execute_reply.started":"2024-08-19T20:37:06.142502Z","shell.execute_reply":"2024-08-19T20:37:24.518591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3 Creating our Train and Dev Sets\nWe are going to choose a 90%, 10% split for our train and dev sets. ","metadata":{}},{"cell_type":"code","source":"df_encoded_x = df_encoded.drop(columns=['class_e', 'class_p'])\ndf_encoded_y = df_encoded['class_e']","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:37:24.522714Z","iopub.execute_input":"2024-08-19T20:37:24.523134Z","iopub.status.idle":"2024-08-19T20:37:25.893718Z","shell.execute_reply.started":"2024-08-19T20:37:24.523093Z","shell.execute_reply":"2024-08-19T20:37:25.892356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_x, temp_data_x, train_data_y, temp_data_y = train_test_split(df_encoded_x, df_encoded_y, test_size=0.02, random_state=16)\ndev_data_x, test_data_x, dev_data_y, test_data_y = train_test_split(temp_data_x, temp_data_y, test_size=.5, random_state=16)\n\nprint(f\"Train set size: {len(train_data_x)} rows\")\nprint(f\"Dev set size: {len(dev_data_x)} rows\")\nprint(f\"Test set size: {len(test_data_x)} rows\")","metadata":{"execution":{"iopub.status.busy":"2024-08-19T21:13:16.231060Z","iopub.execute_input":"2024-08-19T21:13:16.231609Z","iopub.status.idle":"2024-08-19T21:13:31.146642Z","shell.execute_reply.started":"2024-08-19T21:13:16.231566Z","shell.execute_reply":"2024-08-19T21:13:31.145467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4 Initial Model Testing\n## 4.1 Define a custom MCC metric function","metadata":{}},{"cell_type":"code","source":"def mcc(y_true, y_pred):\n    y_pred_pos = tf.round(tf.clip_by_value(y_pred, 0, 1))\n    y_pred_neg = 1 - y_pred_pos\n\n    y_true_pos = tf.round(tf.clip_by_value(y_true, 0, 1))\n    y_true_neg = 1 - y_true_pos\n\n    y_pred_pos = tf.cast(y_pred_pos, tf.float32)\n    y_pred_neg = tf.cast(y_pred_neg, tf.float32)\n    y_true_pos = tf.cast(y_true_pos, tf.float32)\n    y_true_neg = tf.cast(y_true_neg, tf.float32)\n\n    tp = tf.reduce_sum(y_true_pos * y_pred_pos)\n    tn = tf.reduce_sum(y_true_neg * y_pred_neg)\n    fp = tf.reduce_sum(y_true_neg * y_pred_pos)\n    fn = tf.reduce_sum(y_true_pos * y_pred_neg)\n\n    numerator = (tp * tn) - (fp * fn)\n    denominator = tf.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n\n    mcc = tf.where(tf.math.equal(denominator, 0), 0.0, numerator / denominator)\n    return mcc","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:41:58.228868Z","iopub.execute_input":"2024-08-19T20:41:58.230031Z","iopub.status.idle":"2024-08-19T20:41:58.240854Z","shell.execute_reply.started":"2024-08-19T20:41:58.229988Z","shell.execute_reply":"2024-08-19T20:41:58.239544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.2 Define a DNN","metadata":{}},{"cell_type":"code","source":"model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(train_data_x.shape[1],)),  # Explicit Input layer\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=[mcc])","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:42:00.824089Z","iopub.execute_input":"2024-08-19T20:42:00.824666Z","iopub.status.idle":"2024-08-19T20:42:00.869516Z","shell.execute_reply.started":"2024-08-19T20:42:00.824630Z","shell.execute_reply":"2024-08-19T20:42:00.868231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DNN = model.fit(train_data_x, train_data_y, epochs=5, batch_size=128, validation_data = (dev_data_x, dev_data_y))\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:42:01.367314Z","iopub.execute_input":"2024-08-19T20:42:01.367745Z","iopub.status.idle":"2024-08-19T20:47:05.519752Z","shell.execute_reply.started":"2024-08-19T20:42:01.367712Z","shell.execute_reply":"2024-08-19T20:47:05.518434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5 Initial Model Validation\n## 5.1 Loss","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(DNN.history['loss'], label='Training Loss')\nplt.plot(DNN.history['val_loss'], label='Validation Loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:47:05.522908Z","iopub.execute_input":"2024-08-19T20:47:05.523344Z","iopub.status.idle":"2024-08-19T20:47:05.885987Z","shell.execute_reply.started":"2024-08-19T20:47:05.523308Z","shell.execute_reply":"2024-08-19T20:47:05.884683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.2 MCC Metric","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.plot(DNN.history['mcc'], label='Training Accuracy')\nplt.plot(DNN.history['val_mcc'], label='Validation Accuracy')\nplt.title('Training and Validation MCC')\nplt.xlabel('Epochs')\nplt.ylabel('MCC')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T20:47:05.887551Z","iopub.execute_input":"2024-08-19T20:47:05.887930Z","iopub.status.idle":"2024-08-19T20:47:06.274592Z","shell.execute_reply.started":"2024-08-19T20:47:05.887900Z","shell.execute_reply":"2024-08-19T20:47:06.273294Z"},"trusted":true},"execution_count":null,"outputs":[]}]}